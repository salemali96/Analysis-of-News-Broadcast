{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import math\n",
    "from skimage import data\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.patches import Circle\n",
    "from skimage import filters\n",
    "from scipy import signal\n",
    "from skimage.filters.rank import gradient\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy import linalg as LA\n",
    "from skimage.transform import pyramid_gaussian\n",
    "from skimage.morphology import disk\n",
    "from collections import OrderedDict\n",
    "from skimage import transform as tf\n",
    "from scipy.spatial import distance\n",
    "from random import randint\n",
    "import glob\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "import dlib\n",
    "import face_recognition\n",
    "from matplotlib.pyplot import imshow, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input an image, the right point of the face rPt, and the left point of the face lPt.\n",
    "# I add 20 pixels around the face and crop and return the image.\n",
    "def crop_img(img,rPt,lPt):\n",
    "    \n",
    "    x1=rPt[1]+20\n",
    "    y1=rPt[0]-20\n",
    "    x2=lPt[1]-20\n",
    "    y2=lPt[0]+20\n",
    "\n",
    "    crop_img1 = img[y1:y2, x2:x1]\n",
    "\n",
    "    return crop_img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes a frame of a video as input.\n",
    "# I use the facefeatures list to add all the face features of the frame to it.\n",
    "# I get the locations of the faces using the face_recognition.face_locations().\n",
    "# I loop over the faces and crop them using the crop function.\n",
    "# Then, I get the face features from the cropped image.\n",
    "# If face features were found, I calculate the left eye, right eye, nose and mouth and\n",
    "# add them to the features list.\n",
    "\n",
    "def get_feat(frame):\n",
    "\n",
    "    facefeatures = []\n",
    "        \n",
    "    img = face_recognition.load_image_file(frame)\n",
    "    face_location = face_recognition.face_locations(img)\n",
    "\n",
    "    for face in face_location:\n",
    "        features = []\n",
    "\n",
    "        y1 = face[0]\n",
    "        x1 = face[1]\n",
    "        y2 = face[2]\n",
    "        x2 = face[3]\n",
    "\n",
    "        rPt = [y1,x1]\n",
    "        lPt = [y2,x2]\n",
    "\n",
    "        cropped = crop_img(img,rPt,lPt)\n",
    "\n",
    "        faceFeat = face_recognition.face_landmarks(cropped)\n",
    "\n",
    "        if (faceFeat):\n",
    "            \n",
    "            # calculating left and right eyes using the corner points\n",
    "            le_x = (faceFeat[0]['left_eye'][3][0]+faceFeat[0]['left_eye'][0][0])/2\n",
    "            le_y = (faceFeat[0]['left_eye'][3][1]+faceFeat[0]['left_eye'][0][1])/2\n",
    "            leftEye = [le_x,le_y]\n",
    "\n",
    "            re_x = (faceFeat[0]['right_eye'][3][0]+faceFeat[0]['right_eye'][0][0])/2\n",
    "            re_y = (faceFeat[0]['right_eye'][3][1]+faceFeat[0]['right_eye'][0][1])/2\n",
    "            rightEye = [re_x,re_y]\n",
    "            \n",
    "            # calculating the nose by getting the x coordinate of the nose and\n",
    "            # by getting the y coordinate and subtracting 10 pixels so we get\n",
    "            # 10 pixels above the nose tip\n",
    "            n_x = faceFeat[0]['nose_tip'][2][0]\n",
    "            n_y = faceFeat[0]['nose_tip'][2][1]-10\n",
    "            nose = [n_x,n_y]\n",
    "            \n",
    "            # calculating the mouth by averging the first point of the top and\n",
    "            # the first point of the bottom lips\n",
    "            m_x = (faceFeat[0]['top_lip'][0][0]+faceFeat[0]['bottom_lip'][0][0])/2\n",
    "            m_y = (faceFeat[0]['top_lip'][0][1]+faceFeat[0]['bottom_lip'][0][1])/2\n",
    "            mouth = [m_x,m_y]\n",
    "\n",
    "            features.append(leftEye)\n",
    "            features.append(rightEye)\n",
    "            features.append(nose)\n",
    "            features.append(mouth)     \n",
    "            facefeatures.append(features)\n",
    "    return facefeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter:  test_data is the features of each image\n",
    "    \n",
    "# We go throught mat files for males and females \n",
    "# We append the features (left eye, right eye, nose and mouth points )to mf_data list\n",
    "# we identify each feature for males and females\n",
    "# we train the data using Quadratic Discriminant Analysis classifier \n",
    "# to predict whether the face is male or female\n",
    "# we return a list with prediction (males, females or none)\n",
    "\n",
    "\n",
    "def get_pred(test_data):\n",
    "    mf_data = []\n",
    "    mf_gender = []\n",
    "\n",
    "    male = glob.glob('.\\\\train_data\\\\male\\\\*.mat')\n",
    "    female = glob.glob('.\\\\train_data\\\\female\\\\*.mat')\n",
    "    \n",
    "    # Extracting face features from the mat files and adding them to mf_data list \n",
    "    # and adding the corresponding gender to mf_gender list\n",
    "    for i in range(100,200):\n",
    "        mat_m = scipy.io.loadmat(male[i])\n",
    "        mat_f = scipy.io.loadmat(female[i])\n",
    " \n",
    "        mf_data.append([[mat_m['x'][0][0],mat_m['y'][0][0]],\n",
    "                          [mat_m['x'][1][0],mat_m['y'][1][0]],\n",
    "                          [mat_m['x'][2][0],mat_m['y'][2][0]],\n",
    "                          [mat_m['x'][3][0],mat_m['y'][3][0]]])\n",
    "\n",
    "        mf_gender.append('male')\n",
    "\n",
    "        mf_data.append([[mat_f['x'][0][0],mat_f['y'][0][0]],\n",
    "                            [mat_f['x'][1][0],mat_f['y'][1][0]],\n",
    "                            [mat_f['x'][2][0],mat_f['y'][2][0]],\n",
    "                            [mat_f['x'][3][0],mat_f['y'][3][0]]])\n",
    "\n",
    "        mf_gender.append('female')\n",
    "\n",
    "    rfc_prediction = ['None']\n",
    "    \n",
    "    X=np.array(mf_data)\n",
    "    X=X.reshape(X.shape[0], -1)\n",
    "    Y=np.array(mf_gender)\n",
    "    if (len(test_data) > 0):\n",
    "        T=np.array(test_data)\n",
    "        T=T.reshape(T.shape[0], -1)\n",
    "\n",
    "        # QuadraticDiscriminantAnalysis classifier\n",
    "        rfc_clf = QuadraticDiscriminantAnalysis()\n",
    "        rfc_clf.fit(X,Y)\n",
    "        rfc_prediction = rfc_clf.predict(T)\n",
    "\n",
    "    return rfc_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters: \n",
    "# filename: path name for each clip\n",
    "# logoPic: logo pic for each clip\n",
    "\n",
    "# we use all the pathnames for each clip to go through each frame in the clip\n",
    "# we find the face features for each frame using get_feat function \n",
    "# we use it as test data in the training model \n",
    "# we predict the test data whether each face is male or female using get_pred function\n",
    "# Then we match logoPic with the logo in the image if it exists we draw a rectangle around it\n",
    "# Using face_recognition library we detect each face in the image using face_locations function\n",
    "# we detect each face in the picture and draw a rectangle around it and check if it is male we draw\n",
    "# a blue rectangle, if it is female we draw a red rectangle.\n",
    "\n",
    "def detectGenderLogo(filename, logoPic):\n",
    "    # we find all the pathnames for each clip\n",
    "    clip = glob.glob(filename)\n",
    "    logo = cv2.imread(logoPic,0)\n",
    "    \n",
    "    for image in clip:\n",
    "        test_data = get_feat(image)\n",
    "        features = get_pred(test_data)\n",
    "\n",
    "        img = cv2.imread(image)\n",
    "        img_gray = cv2.imread(image,0)\n",
    "\n",
    "        width,length = logo.shape[::-1]\n",
    "        thresh = 0.8\n",
    "        \n",
    "        # match the logo in the image\n",
    "        match = cv2.matchTemplate(img_gray,logo, cv2.TM_CCOEFF_NORMED)\n",
    "        \n",
    "        # find the location point \n",
    "        location = np.where( match >= thresh)\n",
    "        for point in zip(*location[::-1]):\n",
    "            cv2.rectangle(img, point, (point[0] + width, point[1]+ length), (0,255,0),3)\n",
    "        \n",
    "        # Using face_location library we detect each face in the image\n",
    "        faces = face_recognition.face_locations(img)\n",
    "\n",
    "        for i in range(min(len(faces), len(features))):\n",
    "\n",
    "            width = faces[i][1] - faces[i][3]\n",
    "            height = faces[i][2] - faces[i][0]\n",
    "            x = faces[i][3]\n",
    "            y = faces[i][0]\n",
    "            \n",
    "            # We check if the face is male or female and apply a different color for the box\n",
    "            # and gender text\n",
    "            if (features[i] == 'male'):\n",
    "                cv2.rectangle(img, (x,y),(x+width, y+height),(255,0,0),2)\n",
    "                cv2.putText(img,features[i],(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,0,0),2)\n",
    "            elif (features[i] == 'female'):\n",
    "                cv2.rectangle(img, (x,y),(x+width, y+height),(0,0,255),2)\n",
    "                cv2.putText(img,features[i],(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,255),2)\n",
    "            else:\n",
    "                cv2.rectangle(img, (x,y),(x+width, y+height),(0,0,0),2)\n",
    "                cv2.putText(img,features[i],(x,y-5),cv2.FONT_HERSHEY_SIMPLEX,0.8,(0,0,0),2)\n",
    "#         io.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#         show()\n",
    "        cv2.imshow('detected', img)\n",
    "        k = cv2.waitKey(30)\n",
    "        if k ==27:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calling the detectGenderLogo for each clip\n",
    "detectGenderLogo('./clip_1/*.jpg','./cnbc.jpg')\n",
    "detectGenderLogo('./clip_2/*.jpg','./clever_news.jpg')\n",
    "detectGenderLogo('./clip_3/*.jpg','./marvel.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
